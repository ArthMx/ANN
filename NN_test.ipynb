{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created 01/05/2018\n",
    "\n",
    "The goal of this project is to implement a simple Neural Network \n",
    "in Python using numpy.\n",
    "\n",
    "Architecture of the NN : \n",
    "Tanh x 2 times + sigmoid for output\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def ReLU(Z):\n",
    "    '''\n",
    "    Compute the ReLU of the matrix Z\n",
    "    '''\n",
    "    relu = np.maximum(0, Z)\n",
    "    \n",
    "    return relu\n",
    "\n",
    "def Sigmoid(Z):\n",
    "    '''\n",
    "    Compute the sigmoid of the matrix Z\n",
    "    '''\n",
    "    sigmoid = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    return sigmoid\n",
    "\n",
    "def InitializeParameters(n_x, n_1, n_2, n_y):\n",
    "    '''\n",
    "    Initialize the parameters values W and b for each layers.\n",
    "    --------\n",
    "    Input  \n",
    "            - n_x : number of features of X (input of the model)\n",
    "            - n_1 : number of units in layer 1\n",
    "            - n_2 : number of units in layer 2\n",
    "            - n_y : number of features of Y (label for prediction)\n",
    "    Output\n",
    "            - parameters : dictionnary of the parameters W and b\n",
    "                           for each layers\n",
    "    '''\n",
    "    parameters = {}\n",
    "    \n",
    "    parameters['W1'] = np.random.randn(n_1, n_x)*0.01\n",
    "    parameters['b1'] = np.zeros((n_1, 1))\n",
    "    parameters['W2'] = np.random.randn(n_2, n_1)*0.01\n",
    "    parameters['b2'] = np.zeros((n_2, 1))\n",
    "    parameters['W3'] = np.random.randn(n_y, n_2)*0.01\n",
    "    parameters['b3'] = np.zeros((n_y, 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "    \n",
    "def ForwardProp(X, parameters):\n",
    "    '''\n",
    "    Compute the prediction matrix A3.\n",
    "    --------\n",
    "    Input\n",
    "            - X : Matrix of input (n_x, m)\n",
    "            - parameters : dictionnary of parameters W and b, for each layers\n",
    "    Output\n",
    "            - A3 : The prediction matrix (n_y, m)\n",
    "            - cache : Dictionnary of the A and Z, to use them during backprop\n",
    "    '''\n",
    "    # get the parameters from the parameters dict\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # compute forward propagation\n",
    "    # first layer\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    # second layer\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = np.tanh(Z2)\n",
    "    # last layer\n",
    "    Z3 = W3.dot(A2) + b3\n",
    "    A3 = Sigmoid(Z3)\n",
    "    \n",
    "    # get the Z and A in to the cache dictionnary\n",
    "    cache = {}\n",
    "    cache['Z1'] = Z1\n",
    "    cache['A1'] = A1\n",
    "    cache['Z2'] = Z2\n",
    "    cache['A2'] = A2\n",
    "    \n",
    "    return A3, cache\n",
    "\n",
    "def ComputeCost(Y, A3):\n",
    "    '''\n",
    "    Compute the cost function.\n",
    "    --------\n",
    "    Input\n",
    "            - Y : Target matrix (n_y, m)\n",
    "            - A3 : Prediction matrix (n_y, m)\n",
    "    Output\n",
    "            - cost : the cost function computed for Y and A3\n",
    "    '''\n",
    "    # compute the loss matrix \n",
    "    loss = - Y * np.log(A3) - (1-Y) * np.log(1 - A3)\n",
    "    # sum the loss through the m examples\n",
    "    cost = np.average(loss)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def BackProp(X, Y, A3, parameters, cache):\n",
    "    '''\n",
    "    Compute the gradients of the cost for the parameters W, b of each layers\n",
    "    --------\n",
    "    Input\n",
    "            - X :\n",
    "            - Y :\n",
    "            - A3 :\n",
    "            - parameters : \n",
    "            - cache :\n",
    "    Output\n",
    "            - grads : dictionnary of the derivatives of the cost function\n",
    "                      for each parameters\n",
    "    '''\n",
    "    # m = number of training examples\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # get the A and Z from cache\n",
    "    Z1 = cache['Z1']\n",
    "    A1 = cache['A1']\n",
    "    Z2 = cache['Z2']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # get the W parameters\n",
    "    W3 = parameters['W3']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # last layer\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    # second layer\n",
    "    dA2 = W3.T.dot(dZ3)\n",
    "    dZ2 = (1 - np.tanh(Z2)**2) * dA2\n",
    "    dW2 = (1/m) * dZ2.dot(A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    # first layer\n",
    "    dA1 = W2.T.dot(dZ2)\n",
    "    dZ1 = (1 - np.tanh(Z1)**2) * dA1\n",
    "    dW1 = (1/m) * dZ1.dot(X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {}\n",
    "    grads['dW1'] = dW1\n",
    "    grads['db1'] = db1\n",
    "    grads['dW2'] = dW2\n",
    "    grads['db2'] = db2\n",
    "    grads['dW3'] = dW3\n",
    "    grads['db3'] = db3\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def UpdateParameters(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    Update the parameters by gradient descent\n",
    "    ---------\n",
    "    Input\n",
    "            - parameters : dictionnary of parameters W, b of each layer\n",
    "            - grads : dictionnary of gradient of the cost function\n",
    "                      for each parameters W, b of each leayer\n",
    "            - learning_rate : learning rate to use for updating the parameters\n",
    "    Output\n",
    "            - parameters : parameters updated after gradient descent\n",
    "    '''\n",
    "    # get parameters\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # get gradients\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    dW3 = grads['dW3']\n",
    "    db3 = grads['db3']\n",
    "    \n",
    "    # update the parameters    \n",
    "    parameters['W1'] = W1 - learning_rate * dW1\n",
    "    parameters['b1'] = b1 - learning_rate * db1\n",
    "    parameters['W2'] = W2 - learning_rate * dW2\n",
    "    parameters['b2'] = b2 - learning_rate * db2\n",
    "    parameters['W3'] = W3 - learning_rate * dW3\n",
    "    parameters['b3'] = b3 - learning_rate * db3\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def NN_model(X, Y, n_1, n_2, epoch=10000, learning_rate=0.01, verbose=True):\n",
    "    '''\n",
    "    Train a Neural Network of 3 layers (2 hidden layers Tanh and sigmoid for the output).\n",
    "    ----------\n",
    "    Input\n",
    "            - X : input training dataset (m, n_x)\n",
    "            - Y : target of the training dataset (m, 1)\n",
    "            - layer_units : tuple of number of units for the 2 ReLU layers\n",
    "            - epoch : number of iteration\n",
    "            - learning_rate : learning rate for the gradient descent\n",
    "            - verbose : if True, print cost function value every 100 epoch\n",
    "    Output\n",
    "            - parameters : dictionnary of the trained parameters W, b for each layers\n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    # transpose X and Y\n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "    \n",
    "    # get the number of features n_x and number of examples m of X\n",
    "    n_x, m = X.shape\n",
    "    \n",
    "    # initialize the parameters\n",
    "    parameters = InitializeParameters(n_x, n_1, n_2, 1)\n",
    "    \n",
    "    # initialize a list to plot the evolution of the cost function\n",
    "    cost_list = []\n",
    "    for i in range(epoch):\n",
    "        # compute the forward propagation\n",
    "        A3, cache = ForwardProp(X, parameters)\n",
    "        \n",
    "        # compute the back propagation\n",
    "        grads = BackProp(X, Y, A3, parameters, cache)\n",
    "        \n",
    "        # update the parameters\n",
    "        parameters = UpdateParameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if  i%10 == 0:\n",
    "            # compute the cost function\n",
    "            cost = ComputeCost(Y, A3)\n",
    "            cost_list.append(cost)\n",
    "            \n",
    "            if verbose and (i%1000 == 0):\n",
    "                print('Cost function after epoch {} : {}'.format(i, cost))\n",
    "    \n",
    "    print('Cost function after epoch {} : {}'.format(epoch, cost))\n",
    "    print('Time : %.3f s' % (time.time()-t0))\n",
    "    \n",
    "    # print the cost function for each iterations\n",
    "    plt.plot(cost_list)\n",
    "    plt.title('Cost function')\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Cost Function')\n",
    "    \n",
    "    return parameters, cost_list\n",
    "\n",
    "def MakePrediction(X, parameters):\n",
    "    '''\n",
    "    Make prediction of the data X\n",
    "    ---------\n",
    "    Input\n",
    "            - X : Input data (m, n_x)\n",
    "            - parameters : parameters W, b of each layers of the NN model\n",
    "    Output\n",
    "            - Y_pred : Predicted labels for X (m, n_y)\n",
    "    '''\n",
    "    X = X.T\n",
    "    A3, _ = ForwardProp(X, parameters)\n",
    "    Y_pred = (A3 > 0.5)*1\n",
    "    Y_pred = Y_pred.T # transpose the prediction to get the usual form (m, n_y)\n",
    "    \n",
    "    return Y_pred \n",
    "\n",
    "def GetAccuracy(Y, Y_pred):\n",
    "    '''\n",
    "    Compute the accuracy\n",
    "    --------\n",
    "    Input\n",
    "            - Y : Target labels (m, n_y)\n",
    "            - Y_pred : Predicted labels (m, n_y)\n",
    "    Output\n",
    "            - accuracy : accuracy value\n",
    "    '''\n",
    "    accuracy = np.average(Y == Y_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on digits\n",
    "\n",
    "Test of the NN on the digits dataset from scikit-learn. We'll try to predict if the picture is **1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (1797, 64)\n",
      "Y shape : (1797, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits['data']/16 # normalize the values between 0 and 1\n",
    "Y = digits['target']\n",
    "Y = (Y == 1)*1  # target value =1 for digit 1, and 0 for all other digits\n",
    "\n",
    "Y = Y.reshape(-1, 1)\n",
    "print('X shape :', X.shape)\n",
    "print('Y shape :', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS8AAAE/CAYAAAAXCEVDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x260c88994a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "index = 12\n",
    "digit = X[index].reshape(8,8)\n",
    "target = Y[index].squeeze()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(5,5)\n",
    "ax.imshow(digit, cmap='gray_r')\n",
    "ax.set_title('y = {}'.format(target))\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model to predict images of **1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function after epoch 0 : 0.6931670256815936\n",
      "Cost function after epoch 1000 : 0.33240099694801\n",
      "Cost function after epoch 2000 : 0.041878992612856915\n",
      "Cost function after epoch 3000 : 0.02577352503150005\n",
      "Cost function after epoch 4000 : 0.019150243184634867\n",
      "Cost function after epoch 5000 : 0.015442587531780919\n",
      "Time : 7.612 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XucXHV9//HXe2Z2N7ubzXU3CSSB\nBAg3EUEWvNGqqC20CrYqglj1py29iFpvLdSWWvr79SKt1we2oq23qoj3YNEo12pbIQsiECAQQyAx\nkGzu992d3c/vj3N2M9nM7E4uZ2d35/18POYx53zPd875nCXMZ77ne873q4jAzMwMIFfrAMzMbPxw\nUjAzsyFOCmZmNsRJwczMhjgpmJnZECcFMzMb4qRgdggk/bGkDZJ2SZo9hsf9C0mfG6vjWf1yUrAJ\nSdKbJHWlX87PSPqBpPOPcJ9rJL1yhO0NwEeB34iIqRGx+UiON8JxXiZpXWlZRPxdRPx+FsczK+Wk\nYBOOpPcBHwf+DpgLHAd8Grgk40PPBaYAKzI+jlnNOCnYhCJpOnAd8M6I+HZE7I6Ivoi4JSI+mNZp\nkvRxSevT18clNaXb2iV9X9I2SVsk/URSTtKXSZLLLWnr48+GHfdkYGW6uk3SHZIWSQpJhZJ6d0n6\n/XT5bZJ+KumfJG2V9KSki0rqzpL0+TTGrZK+K6kV+AFwbBrHLknHSvqwpP8o+ezFklak53GXpNNK\ntq2R9AFJD0raLunrkqYc3f8SNlk5KdhE8yKSX+vfGaHOh4AXAmcBzwPOA/4y3fZ+YB3QQfLL/y+A\niIjfA54GXpNeGvpI6Q4j4nHgOenqjIi4oMp4X0CSTNqBjwD/Jknpti8DLel+5wAfi4jdwEXA+jSO\nqRGxvnSHaYL6GvCn6XncSpLMGkuqXQpcCCwGzgTeVmW8VuecFGyimQ1siojiCHWuAK6LiI0R0Q38\nDfB76bY+4Bjg+LSF8ZPIdgCwpyLisxHRD3wxPfZcSceQfPn/UURsTWO5u8p9vhH4z4j4cUT0Af8E\nNAMvLqnzyYhYHxFbgFtIEqTZqJwUbKLZDLSXXrIp41jgqZL1p9IygOuBVcCPJK2WdHU2YQ55dnAh\nIvaki1OBhcCWiNh6GPs84PwiYgBYC8wvd1xgT3pMs1E5KdhE87/APuC1I9RZDxxfsn5cWkZE7IyI\n90fECcBrgPdJekVa71BbDLvT95aSsnlVfnYtMEvSjDLbRovjgPNLL0ctBH5V5bHNKnJSsAklIrYD\n1wI3SHqtpBZJDZIukjTYD/A14C8ldUhqT+v/B4CkV0s6Kf0i3QH0py+ADcAJhxBLN8kX8Zsl5SW9\nHTixys8+Q9Kh/GlJM9Nz+PWSOGannerl3Az8tqRXpLfJvh/oAf6n2tjNKnFSsAknIj4KvI+k87ib\n5Ff3VcB30yr/F+gCHgQeAu5PywCWALcBu0haHZ+OiLvSbX9Pkky2SfpAleH8AfBBkstaz+HQvph/\nj6SP4zFgI0nHMRHxGEliW53GcmzphyJiJfBm4FPAJpIWz2siovcQjm1WljzJjpmZDXJLwczMhjgp\nmJnZECcFMzMb4qRgZmZDRnoAaFxqb2+PRYsW1ToMM7MJ5b777tsUER2j1ZtwSWHRokV0dXXVOgwz\nswlF0lOj1/LlIzMzK+GkYGZmQ5wUzMxsSKZJQdKFklZKWlVuNEpJH5P0QPp6XNK2LOMxM7ORZdbR\nLCkP3AC8imRSk+WSlkbEI4N1IuK9JfXfBZydVTxmZja6LFsK5wGrImJ1OlDXTYw8h+7lJIOAmZlZ\njWSZFOaTjF45aB0HTgIyRNLxJNMG3lFh+5WSuiR1dXd3H/VAzcwskWVSUJmySkOyXgZ8M52y8OAP\nRdwYEZ0R0dnRMeqzF2UtX7OFf/7RSvr6Bw7r82Zm9SDLpLCOZDaoQQtIZ78q4zIyvnT086e38qk7\nVtFbdFIwM6sky6SwHFgiabGkRpIv/qXDK0k6BZhJMuFJZvK55FSLA54/wsysksySQkQUSWbDWgY8\nCtwcESskXSfp4pKqlwM3Rcaz/RRyydWsoi8fmZlVlOnYRxFxK3DrsLJrh61/OMsYBhXySVLod0vB\nzKyiunmieail4KRgZlZR3SSFoT6FficFM7NK6iYpNOQHWwruUzAzq6RukkLel4/MzEZVN0lh/91H\nTgpmZpXUUVJITtV3H5mZVVY3SSGf9in0uU/BzKyiukkKg5eP3FIwM6usjpKCb0k1MxtN/SQF35Jq\nZjaqukkKviXVzGx0dZMUGgbvPvLlIzOziuomKexvKfjykZlZJXWTFPb3KbilYGZWSf0kBd+SamY2\nqjpKCsmp9rlPwcysovpJCkOT7LhPwcyskvpJCr4l1cxsVHWTFPIeJdXMbFR1kxQK+XSYC7cUzMwq\nyjQpSLpQ0kpJqyRdXaHOpZIekbRC0lezimX/3UfuUzAzq6SQ1Y4l5YEbgFcB64DlkpZGxCMldZYA\n1wAviYitkuZkFc/g5SPffWRmVlmWLYXzgFURsToieoGbgEuG1fkD4IaI2AoQERuzCqYh70l2zMxG\nk2VSmA+sLVlfl5aVOhk4WdJ/S/qZpAvL7UjSlZK6JHV1d3cfVjBpQ8F9CmZmI8gyKahM2fBv5AKw\nBHgZcDnwOUkzDvpQxI0R0RkRnR0dHYcXjEQhJ4r97lMwM6sky6SwDlhYsr4AWF+mzvcioi8ingRW\nkiSJTBTy8uUjM7MRZJkUlgNLJC2W1AhcBiwdVue7wMsBJLWTXE5anVVAhVzOl4/MzEaQWVKIiCJw\nFbAMeBS4OSJWSLpO0sVptWXAZkmPAHcCH4yIzVnFlPflIzOzEWV2SypARNwK3Dqs7NqS5QDel74y\n15CXWwpmZiOomyeaIWkpuE/BzKyyukoKhVzOD6+ZmY2gvpJCXh7mwsxsBPWVFHJyS8HMbAR1lRQa\nC3l6im4pmJlVUmdJIUevb0k1M6uorpJCUz5Hb7G/1mGYmY1bdZUUGgs5en35yMysovpLCr58ZGZW\nUX0lhXyOnj4nBTOzSuoqKTQ1uKVgZjaSukoKjXn3KZiZjaS+koI7ms3MRuSkYGZmQ+ouKfS4T8HM\nrKK6SgpNaZ9CMo2DmZkNV1dJobGQnK4HxTMzK68uk0KPh7owMyurvpJCPjlddzabmZVXV0mhqSEP\n4AfYzMwqyDQpSLpQ0kpJqyRdXWb72yR1S3ogff1+lvG4pWBmNrJCVjuWlAduAF4FrAOWS1oaEY8M\nq/r1iLgqqzhKDfYpOCmYmZWXZUvhPGBVRKyOiF7gJuCSDI83qv0dzU4KZmblZJkU5gNrS9bXpWXD\nvU7Sg5K+KWlhhvHsbym4T8HMrKwsk4LKlA1/QOAWYFFEnAncBnyx7I6kKyV1Serq7u4+7ICa3Kdg\nZjaiLJPCOqD0l/8CYH1phYjYHBE96epngXPK7SgiboyIzojo7OjoOOyABu8+2tvn5xTMzMrJMiks\nB5ZIWiypEbgMWFpaQdIxJasXA49mGA8tjWlS6HVSMDMrJ7O7jyKiKOkqYBmQB/49IlZIug7oioil\nwLslXQwUgS3A27KKB6C1MTndPU4KZmZlZZYUACLiVuDWYWXXlixfA1yTZQylmodaCsWxOqSZ2YRS\nV080D14+2u2WgplZWXWVFJrTjmZfPjIzK6+ukkIuJ5ob8r58ZGZWQV0lBYDWprxbCmZmFdRdUmhu\ndFIwM6ukqruPJL0YWFRaPyK+lFFMmWppKLDHl4/MzMoaNSlI+jJwIvAAMPgTO4CJmRR8+cjMrKJq\nWgqdwOkxSWa7b/HlIzOziqrpU3gYmJd1IGOluaHgpGBmVkE1LYV24BFJ9wKDg9cRERdnFlWGkruP\n3KdgZlZONUnhw1kHMZamNhXYuc9JwcysnFGTQkTcLWkucG5adG9EbMw2rOzMaGlg+94+IgKp3JQP\nZmb1a9Q+BUmXAvcCbwAuBe6R9PqsA8vK9OYG+gfC4x+ZmZVRzeWjDwHnDrYOJHWQzJL2zSwDy8r0\n5gYAtu3pZWpTpoPEmplNONXcfZQbdrloc5WfG5cGk8L2vX01jsTMbPyp5qfyDyUtA76Wrr+RYXMk\nTCTTmxsBJwUzs3Kq6Wj+oKTXAS8BBNwYEd/JPLKMDLUU9jgpmJkNV9VF9Yj4FvCtjGMZE9NbfPnI\nzKySiklB0k8j4nxJO0nGOhraBERETMs8ugwMdTQ7KZiZHaRiUoiI89P3trELJ3utjXmaCjk27+oZ\nvbKZWZ2p5jmFL1dTNlFIYu60KWzY4aRgZjZcNbeWPqd0RVIBOKeanUu6UNJKSaskXT1CvddLCkmd\n1ez3SM2d1sTGnfvG4lBmZhNKxaQg6Zq0P+FMSTvS105gA/C90XYsKQ/cAFwEnA5cLun0MvXagHcD\n9xzmORyyOdOmsNEtBTOzg1RMChHx92l/wvURMS19tUXE7Ii4pop9nwesiojVEdEL3ARcUqbe3wIf\nAcbsp/uctiY27nRSMDMbrprLR/dKmj64ImmGpNdW8bn5wNqS9XVp2RBJZwMLI+L7I+1I0pWSuiR1\ndXd3V3Hokc2dNoVdPUV29Xi0VDOzUtUkhb+OiO2DKxGxDfjrKj5XbgjSoVtbJeWAjwHvH21HEXFj\nRHRGRGdHR0cVhx7Z/BnNAKzbuueI92VmNplUNfZRmbJqHnpbBywsWV8ArC9ZbwPOAO6StAZ4IbB0\nLDqbF7e3ArBm0+6sD2VmNqFUkxS6JH1U0omSTpD0MeC+Kj63HFgiabGkRuAyYOngxojYHhHtEbEo\nIhYBPwMujoiuwziPQ7IoTQqrnRTMzA5QTVJ4F9ALfB34BkmH8DtH+1BEFIGrgGXAo8DNEbFC0nWS\najqV59SmAu1Tm9xSMDMbppoB8XYDFZ8xGOWztzJsRNWIuLZC3ZcdzjEO10lzWlm5YddYHtLMbNwb\nNSlIOhn4ALCotH5EXJBdWNk7c8EMvvA/a+gtDtBYmLDTQ5iZHVXVdBh/A/hX4HPApJnD8rnzp9Nb\nHODxDTs5Y/700T9gZlYHqkkKxYj4l8wjGWNnHzcDgJ+t3uykYGaWqua6yS2S/kTSMZJmDb4yjyxj\nC2a2cGJHK3c/fuQPw5mZTRbVtBTemr5/sKQsgBOOfjhj64JT5/CF/1nDlt29zGptrHU4ZmY1N2pL\nISIWl3lN+IQA8LpzFtDXH3z7/nW1DsXMbFyo5u6jt5Qrj4gvHf1wxtap86bxgsWz+Mx/reby846j\ntamq2UnNzCatavoUzi15/RrwYaCmD58dTX9+0als2tXDX333YQYGYvQPmJlNYtU8vPau0vV0xNQJ\nO/PacM8/bibvecUSPn7bE0jiby55DlPdYjCzOnU43357gCVHO5Baes8rlhABn7j9Ce5+vJvXn7OA\nl5w0m+fOn8705gakcgO+mplNPtX0KdzC/iGvcySzqN2cZVBjTRLvfdXJvOyUDj55+xN89ier+de7\nfwlAc0OejrYmpjUXaCrkaSrkaCrkaMjnkECIXC55l5J95ZSMGy5pfx2BBDmJXE7kJfI5kZMo5JP3\nfA7yJdtzuaROIX21NBZoacrT0pinuaFAa1OeWa2NdLQ10VTI1/aPaGaTQjUthX8qWS4CT0XEpLxd\n5+zjZvL5/3Me2/f28fCvtvPI+h1s3LmPjTt72LWvSE9xgJ5iP7t6ivQWB4iAIBgIiAgCkrJIy4h0\nfX/ZQAQDEfQPJK+BIFmOYCB9j8Po2pjR0sDi9lZOO2YaZ86fzgWnzWFO25Sj/jcys8lNUeEbSNIL\nI+JnYxzPqDo7O6OrK/PRtWtqMDkkSSN5L/YHe/r62dNTZE9vP7t7i+zp6Wfz7h427uhhw859PLFh\nF48+s4Md+4pI8KITZnPVBSfx4hPba31KZlZjku6LiFHnqxmppfBp4Pnpzv43Il50tIKzkeVyIodo\nGHZFaGYVn40IVm7YybKHN/DVe5/iTZ+9hze94Dj++jWn+xKTmY1qpKRQ2rvq6xAThCROnTeNU+dN\n4w9fegIfu+1xPnP3arp39vDpK55PQ94jwppZZSN9Q+QkzZQ0u2R50ox9VA+mNOS55qLT+JuLn8OP\nH9nAP/zgsVqHZGbj3Egthekk024OthjuL9k2KcY+qhdvffEiftm9i3/76ZO89qz5PHeBR4U1s/Iq\nthTSuZNPmMxjH9WTD/7mKcxubeQjy9xaMLPKfIG5TrRNaeDt5y/mJ09s4vENO2sdjpmNU04KdeTy\n846jkBPfvG9SPmZiZkdBpklB0oWSVkpaJenqMtv/SNJDkh6Q9FNJp2cZT72b1drIy07p4JZfrKfS\n8ylmVt9GTQqSDhr8rlxZmTp54AbgIpKhMS4v86X/1Yh4bkScBXwE+GhVUdth+43T5/HM9n08+owv\nIZnZwappKTyndCX9sj+nis+dB6yKiNUR0QvcBFxSWiEidpSstrJ/jCXLyEtP6QDgrsc31jgSMxuP\nKiYFSddI2gmcKWlH+toJbAS+V8W+5wNrS9bXpWXDj/NOSb8kaSm8u0IsV0rqktTV3e05lY/E3GlT\nOGnOVJY/uaXWoZjZODTSLal/HxFtwPURMS19tUXE7Ii4pop9lxtv+qCWQETcEBEnAn8O/GWFWG6M\niM6I6Ozo6Kji0DaScxfN5L6ntnpSITM7SDWXj74vqRVA0pslfVTS8VV8bh2wsGR9AbB+hPo3Aa+t\nYr92hJ5/3Ex27Cvyy+5dtQ7FzMaZapLCvwB7JD0P+DPgKaCa+ZmXA0skLZbUCFwGLC2tIKl0sp7f\nBp6oKmo7IoNPNK9Yv2OUmmZWb6pJCsVI7l+8BPhERHwCaBvtQxFRBK4ClgGPAjdHxApJ10kanOP5\nKkkrJD0AvA9462GdhR2SEzum0ljIsWL99lqHYmbjTDWT7OyUdA3we8CvpXcfNVSz84i4Fbh1WNm1\nJcvvOYRY7ShpyOc4dV6bWwpmdpBqWgpvBHqAt0fEsyR3EF2faVSWuZPmTGV19+5ah2Fm48yoSSFN\nBF8Bpkt6NbAvIqrpU7Bx7IT2Vp7dsY/dPcVah2Jm40g1TzRfCtwLvAG4FLhH0uuzDsyydULHVACe\n3OTWgpntV02fwoeAcyNiI4CkDuA24JtZBmbZWtzeCsDqTbs5Y77nVzCzRDV9CrnBhJDaXOXnbBwb\nTApPul/BzEpU01L4oaRlwNfS9TcCP8guJBsLUxryzJ/RzOpNfoDNzPYbNSlExAcl/S5wPsnQFTdG\nxHcyj8wyd0JHq/sUzOwAIw2Id5KklwBExLcj4n0R8V5gs6QTxyxCy8zi9lZWd+/23ApmNmSkvoGP\nA+UG3d+TbrMJ7rhZLezqKbJtT1+tQzGzcWKkpLAoIh4cXhgRXcCizCKyMbNgZgsAa7fuqXEkZjZe\njJQUpoywrfloB2Jjb+Gs5D/juq17axyJmY0XIyWF5ZL+YHihpHcA92UXko2VoZbCFrcUzCwx0t1H\nfwp8R9IV7E8CnUAj8DtZB2bZm97cwLQpBV8+MrMhFZNCRGwAXizp5cAZafF/RsQdYxKZjYmFs1p8\n+cjMhlTznMKdwJ1jEIvVwMKZLTyxsdxNZmZWjzxcRZ1bOKuZdVv3+lkFMwOcFOregpkt9BQH6N7Z\nU+tQzGwccFKoc4O3pa51v4KZ4aRQ9xamt6Wu8x1IZoaTQt2bPzNtKfhZBTMj46Qg6UJJKyWtknR1\nme3vk/SIpAcl3S7p+CzjsYO1NBZon9ro21LNDMgwKUjKAzcAFwGnA5dLOn1YtZ8DnRFxJslMbh/J\nKh6rbMHMFj/AZmZAti2F84BVEbE6InqBm4BLSitExJ0RMfht9DNgQYbxWAULZ7WwdotbCmaWbVKY\nD6wtWV+XllXyDirM6CbpSkldkrq6u7uPYogGsGBmM+u37aV/wM8qmNW7LJOCypSV/daR9GaScZWu\nL7c9Im6MiM6I6Ozo6DiKIRokdyAVB4Jnd+yrdShmVmNZJoV1wMKS9QXA+uGVJL0S+BBwcUT4Caoa\nGHpWwXcgmdW9LJPCcmCJpMWSGoHLgKWlFSSdDXyGJCFszDAWG8GCoWcV3K9gVu8ySwoRUQSuApYB\njwI3R8QKSddJujitdj0wFfiGpAckLa2wO8vQsTOmILmlYGZVjJJ6JCLiVuDWYWXXliy/MsvjW3Wa\nCnnmTZvi21LNzE80W2LBzGZfPjIzJwVLLJzZwjpfPjKre04KBiQPsD2zYx/7+vprHYqZ1ZCTggFw\n0pypRMDq7t21DsXMashJwQA4ZV4bAI9v8NScZvXMScEAWDS7lYa8nBTM6pyTggHQWMixuL3VScGs\nzjkp2JCT57ax0knBrK45KdiQU+a2sXbLXnb3FGsdipnViJOCDTk57Wx2a8Gsfjkp2JDnLZgBwP1P\nba1xJGZWK04KNmTe9CksnNVM1xonBbN65aRgBzh30SyWr9lChGdhM6tHTgp2gHMXzWLz7l6e3OQn\nm83qkZOCHeDcRTMBuPfJLTWOxMxqwUnBDnBix1SOnT6F2x71RHhm9chJwQ4gid88Yx7/9UQ3u/y8\nglndcVKwg1x0xjH0Fge44zG3FszqjZOCHeSc42fS0dbE0gfW1zoUMxtjTgp2kHxOXNq5gNsf28BT\nm30Xklk9yTQpSLpQ0kpJqyRdXWb7r0u6X1JR0uuzjMUOzVtetIhCTnzhf9bUOhQzG0OZJQVJeeAG\n4CLgdOBySacPq/Y08Dbgq1nFYYdn7rQpvObMY7np3rU8u31frcMxszGSZUvhPGBVRKyOiF7gJuCS\n0goRsSYiHgQGMozDDtN7X3Uy/QPBR374WK1DMbMxkmVSmA+sLVlfl5YdMklXSuqS1NXd3X1UgrPR\nLZzVwjt+bTHf/vmv+OkTm2odjpmNgSyTgsqUHdaAOhFxY0R0RkRnR0fHEYZlh+LdFyzhpDlTed/N\nD7B5V0+twzGzjGWZFNYBC0vWFwC+x3GCaW7M84nLzmLb3j5+/0td7O3tr3VIZpahLJPCcmCJpMWS\nGoHLgKUZHs8y8pxjp/PJy87mgbXbuOqr99NbdBeQ2WSVWVKIiCJwFbAMeBS4OSJWSLpO0sUAks6V\ntA54A/AZSSuyiseOzIVnzOO6S87g9sc28o4vLvcQGGaTlCbauPmdnZ3R1dVV6zDq1s1da7nm2w9x\nytw2Pn3F81nU3lrrkMysCpLui4jO0er5iWY7JJd2LuRzb+3kV9v28upP/ZRv3bfOE/KYTSJOCnbI\nXn7KHG59z69x6rw23v+NX/Cmz97DExt21josMzsKnBTssMyf0czX//BF/L/fOYNHntnBRZ/4CX/+\nzQd5evOeWodmZkfAfQp2xLbs7uWTtz/BV+99mv6B4LefewxXvOA4zls8C6nc4ypmNtaq7VNwUrCj\nZsOOfdz4X6u5uWstO/cVOWnOVC7tXMBFZxzDwlkttQ7PrK45KVjN7Okt8v0Hn+Gr9zzNA2u3AXDm\ngulcdMYxXHDqHE6eO9UtCLMx5qRg48LTm/fwg4ef4daHnuEX67YDMKetifNPauf8Je288ITZHDuj\nucZRmk1+Tgo27qzftpefPrGJn6zaxH+v2sSW3b0AzJs2hecfP4OzF87k+cfP4PRjptPcmK9xtGaT\ni5OCjWsDA8Ejz+yga80W7n96Gz9fu5W1W/YCkBMsam/ltGOmcdq8Nk47ZhqnHjONY6dP8WUns8Pk\npGATzsad+3jg6W2sWL+Dx57dwaPP7OTpLftvcW1tzLO4o5XF7VNZ3N7KCe2tLG5vZVF7K9ObG2oY\nudn456Rgk8KuniIrn93BI8/s5Jcbd/Hkpt08uWk367buYaDkn+6s1kYWzGxm/oz0lS4vmNnC/JnN\nThpW96pNCoWxCMbscE1tKnDO8bM45/hZB5T3FPtZu2UPq7uTJLFm827Wbd3Lyg07ueOxjfQMG8m1\nranAvOlTmDOtibltU5gzbQpz2pqYO20Kc6c1Mact2TalwX0ZVt+cFGxCairkOWlOGyfNaTtoW0Sw\neXcvv9q6l3Vb9/KrbXv41da9bNjRw4ad+7jnyS1s3LmPvv6DW8nTmxuY09bE7KmNzG5tYlZrI7Na\nG5k9tXH/clo+s6WBQt6DAtjk4qRgk44k2qc20T61iectnFG2TkSwdU8fG3fuY8OOHjbu2MfGncn7\nhh09bN7dw6PP7mDL7l627emrcJwkiSSJopGZLY1Mb27Y/2pJ3qeVlqWvBicTG6ecFKwuSRr65X/q\nvJHrFvsH2Lqnjy27e9m8u4ctu3uT5V29+5d39/DU5j1s39vH9r197O0beYa6lsb8QYlialOBqVMK\ntDYVkuX01dpUoG14+ZQCLQ15cjnfjWVHl5OC2SgK+RwdbU10tDUBB1+uKqe3ODCUILbv7WNHyXK5\n19Nb9rCrp8juniK7eoplL22VkySNPFObCrQ0FmhuzNPckL4a8wevl7xPacjT0nhw+eC2xkKOvJNO\n3XFSMMtAY6E0kRy6nmI/u/YV2d3Tz86ePnb39LOrp49dPYPlRXYOJpF9RXb1Ftnb28+e3iLb9vbx\n7PZ97Okrsrd3gH19SfnAYdxoWMiJpkKOpoY8jfkcTQ25ZL2QJI2m9NWYliV1czTm86PUzVHI5Wgo\n5GjIi8Z8joZ8jkLJckMhR0NO+5fzoiGXc+soY04KZuNQUyFP09Q8s6cenf1FBL39A+zrHWBvmiT2\n9vWnCaOfvb397O3rTxNLP739A/T0DdBT7Ke3OEBPMVnuKQ4cuN43wK6e4lDd4durbfEcinxOSYLI\n5yokE1HIpdsKaVLJ54Y+U8jlKOREIS8KOZHPJdvyOVHI59IypWX7tzXkcmmdZP/76yTrg/srt49C\nyecKOZHP799fQ17j6qFMJwWzOiAp/SWfZzpj98zGwEAckGBKk0Vf/0D62r/cW0yWiwMD9BWTz/b1\nD1Ds3788/DN96Wd603qDy33pcXftK9JbUr/YHxQHBugfCPr6g/6BZD0pr81zWzkxlFjyaesoSTYH\nJqH3vGIJr3nesZnG4qRgZpnJ5cSUXD59/mP8P0AYMZgk0vc0gRTTsmL/wNC2vv7yiaV0W3FYwikO\nBP39B+9vcHv/wMDB+yuJaUZL9n/DTJOCpAuBTwB54HMR8Q/DtjcBXwLOATYDb4yINVnGZGZWiZT+\nMq/jZxgzu1laUh64AbgIOB29NjONAAAIxUlEQVS4XNLpw6q9A9gaEScBHwP+Mat4zMxsdFk+QXMe\nsCoiVkdEL3ATcMmwOpcAX0yXvwm8QuOpx8XMrM5kmRTmA2tL1telZWXrREQR2A7MHr4jSVdK6pLU\n1d3dnVG4ZmaWZVIo94t/eNd+NXWIiBsjojMiOjs6Oo5KcGZmdrAsk8I6YGHJ+gJgfaU6kgrAdGBL\nhjGZmdkIskwKy4ElkhZLagQuA5YOq7MUeGu6/HrgjphoEzyYmU0imd2SGhFFSVcBy0huSf33iFgh\n6TqgKyKWAv8GfFnSKpIWwmVZxWNmZqPL9DmFiLgVuHVY2bUly/uAN2QZg5mZVW/CTccpqRt46jA/\n3g5sOorhTAQ+5/rgc64PR3LOx0fEqHfqTLikcCQkdVUzR+lk4nOuDz7n+jAW5+zpn8zMbIiTgpmZ\nDam3pHBjrQOoAZ9zffA514fMz7mu+hTMzGxk9dZSMDOzETgpmJnZkLpJCpIulLRS0ipJV9c6nqNF\n0r9L2ijp4ZKyWZJ+LOmJ9H1mWi5Jn0z/Bg9Ken7tIj98khZKulPSo5JWSHpPWj5pz1vSFEn3SvpF\nes5/k5YvlnRPes5fT4eUQVJTur4q3b6olvEfLkl5ST+X9P10fVKfL4CkNZIekvSApK60bMz+bddF\nUqhywp+J6gvAhcPKrgZuj4glwO3pOiTnvyR9XQn8yxjFeLQVgfdHxGnAC4F3pv89J/N59wAXRMTz\ngLOACyW9kGRiqo+l57yVZOIqmDwTWL0HeLRkfbKf76CXR8RZJc8kjN2/7YiY9C/gRcCykvVrgGtq\nHddRPL9FwMMl6yuBY9LlY4CV6fJngMvL1ZvIL+B7wKvq5byBFuB+4AUkT7cW0vKhf+ckY469KF0u\npPVU69gP8TwXpF+AFwDfJxlqf9Keb8l5rwHah5WN2b/tumgpUN2EP5PJ3Ih4BiB9n5OWT7q/Q3qZ\n4GzgHib5eaeXUh4ANgI/Bn4JbItkgio48LyqmsBqnPs48GfAQLo+m8l9voMC+JGk+yRdmZaN2b/t\nTAfEG0eqmsynDkyqv4OkqcC3gD+NiB0jzOQ6Kc47IvqBsyTNAL4DnFauWvo+oc9Z0quBjRFxn6SX\nDRaXqTopzneYl0TEeklzgB9LemyEukf9vOulpVDNhD+TyQZJxwCk7xvT8knzd5DUQJIQvhIR306L\nJ/15A0TENuAukv6UGekEVXDgeU30CaxeAlwsaQ3J/O4XkLQcJuv5DomI9en7RpLkfx5j+G+7XpJC\nNRP+TCalkxe9leSa+2D5W9I7Fl4IbB9skk4kSpoE/wY8GhEfLdk0ac9bUkfaQkBSM/BKkg7YO0km\nqIKDz3nCTmAVEddExIKIWETy/+sdEXEFk/R8B0lqldQ2uAz8BvAwY/lvu9adKmPYefNbwOMk12E/\nVOt4juJ5fQ14Bugj+dXwDpJrqbcDT6Tvs9K6IrkL65fAQ0BnreM/zHM+n6SJ/CDwQPr6rcl83sCZ\nwM/Tc34YuDYtPwG4F1gFfANoSsunpOur0u0n1PocjuDcXwZ8vx7ONz2/X6SvFYPfVWP5b9vDXJiZ\n2ZB6uXxkZmZVcFIwM7MhTgpmZjbEScHMzIY4KZiZ2RAnBas5SSHpn0vWPyDpw0dp31+Q9PrRax7x\ncd6Qjtp657DyYyV9M10+S9JvHcVjzpD0J+WOZXa4nBRsPOgBfldSe60DKZWOrlutdwB/EhEvLy2M\niPURMZiUziJ5nuJQYhhpKJoZwFBSGHYss8PipGDjQZFk7tn3Dt8w/Je+pF3p+8sk3S3pZkmPS/oH\nSVcomXPgIUknluzmlZJ+ktZ7dfr5vKTrJS1Px6H/w5L93inpqyQPAw2P5/J0/w9L+se07FqSB+r+\nVdL1w+ovSus2AtcBb0zHyX9j+vTqv6cx/FzSJeln3ibpG5JuIRkYbaqk2yXdnx77knT3/wCcmO7v\n+sFjpfuYIunzaf2fS3p5yb6/LemHSsbm/0jJ3+MLaawPSTrov4XVh3oZEM/GvxuABwe/pKr0PJJB\n4bYAq4HPRcR5SibdeRfwp2m9RcBLgROBOyWdBLyFZEiAcyU1Af8t6Udp/fOAMyLiydKDSTqWZJz+\nc0jG8v+RpNdGxHWSLgA+EBFd5QKNiN40eXRGxFXp/v6OZDiGt6dDWNwr6bb0Iy8CzoyILWlr4Xci\nGfSvHfiZpKUkY+qfERFnpftbVHLId6bHfa6kU9NYT063nUUysmwPsFLSp0hG3ZwfEWek+5ox8p/e\nJiu3FGxciIgdwJeAdx/Cx5ZHxDMR0UPymP/gl/pDJIlg0M0RMRART5Akj1NJxpR5i5KhqO8hGUZg\nSVr/3uEJIXUucFdEdEcyPPNXgF8/hHiH+w3g6jSGu0iGajgu3fbjiBgc0E3A30l6ELiNZGjkuaPs\n+3zgywAR8RjwFDCYFG6PiO0RsQ94BDie5O9ygqRPSboQ2HEE52UTmFsKNp58nGTymM+XlBVJf7xI\nEtBYsq2nZHmgZH2AA/9tDx/LJUi+aN8VEctKNygZpnl3hfgqjs19mAS8LiJWDovhBcNiuALoAM6J\niD4lI4dOqWLflZT+3fpJJq3ZKul5wG+StDIuBd5e1VnYpOKWgo0b6S/jm9k/xSIks1Cdky5fAjQc\nxq7fICmX9jOcQDI71TLgj5UMwY2kk9NRKUdyD/BSSe1pJ/TlwN2HEMdOoK1kfRnwrjTZIensCp+b\nTjK3QF/aN3B8hf2V+i+SZEJ62eg4kvMuK70slYuIbwF/BUy4eazt6HBSsPHmn4HSu5A+S/JFfC/J\n9JOVfsWPZCXJl/cPgD9KL5t8juTSyf1p5+xnGKXlHMmQxNeQDN/8C+D+iPjeSJ8Z5k7g9MGOZuBv\nSZLcg2kMf1vhc18BOpVM4n4F8Fgaz2aSvpCHh3dwA58G8pIeAr4OvC29zFbJfOCu9FLWF9LztDrk\nUVLNzGyIWwpmZjbEScHMzIY4KZiZ2RAnBTMzG+KkYGZmQ5wUzMxsiJOCmZkN+f+bvJEEJnSMVQAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x260c7a2e358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_1, n_2 = 20, 10\n",
    "\n",
    "parameters, cost_list = NN_model(X_train, Y_train, n_1, n_2, epoch=5000, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set accuracy : 0.997216423104\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      1283\n",
      "          1       0.99      0.99      0.99       154\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_train_pred = MakePrediction(X_train, parameters)\n",
    "accuracy = GetAccuracy(Y_train, Y_train_pred)\n",
    "\n",
    "print('train set accuracy :', accuracy)\n",
    "print(classification_report(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy : 0.986111111111\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99       332\n",
      "          1       0.85      1.00      0.92        28\n",
      "\n",
      "avg / total       0.99      0.99      0.99       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = MakePrediction(X_test, parameters)\n",
    "accuracy = GetAccuracy(Y_test, Y_test_pred)\n",
    "\n",
    "print('test set accuracy :', accuracy)\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Very good performance on this small dataset !\n",
    "Next : implement a softmax classifier for the output, to predict every digits possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
