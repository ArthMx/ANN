{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created 01/05/2018\n",
    "\n",
    "The goal of this project is to implement a simple Neural Network \n",
    "in Python using numpy.\n",
    "\n",
    "Architecture of the NN : \n",
    "Tanh x 2 times + sigmoid for output\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def ReLU(Z):\n",
    "    '''\n",
    "    Compute the ReLU of the matrix Z\n",
    "    '''\n",
    "    relu = np.maximum(0, Z)\n",
    "    \n",
    "    return relu\n",
    "\n",
    "def Sigmoid(Z):\n",
    "    '''\n",
    "    Compute the sigmoid of the matrix Z\n",
    "    '''\n",
    "    sigmoid = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    return sigmoid\n",
    "\n",
    "def InitializeParameters(n_x, n_1, n_2, n_y):\n",
    "    '''\n",
    "    Initialize the parameters values W and b for each layers.\n",
    "    --------\n",
    "    Input  \n",
    "            - n_x : number of features of X (input of the model)\n",
    "            - n_1 : number of units in layer 1\n",
    "            - n_2 : number of units in layer 2\n",
    "            - n_y : number of features of Y (label for prediction)\n",
    "    Output\n",
    "            - parameters : dictionnary of the parameters W and b\n",
    "                           for each layers\n",
    "    '''\n",
    "    parameters = {}\n",
    "    \n",
    "    parameters['W1'] = np.random.randn(n_1, n_x)*0.01\n",
    "    parameters['b1'] = np.zeros((n_1, 1))\n",
    "    parameters['W2'] = np.random.randn(n_2, n_1)*0.01\n",
    "    parameters['b2'] = np.zeros((n_2, 1))\n",
    "    parameters['W3'] = np.random.randn(n_y, n_2)*0.01\n",
    "    parameters['b3'] = np.zeros((n_y, 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "    \n",
    "def ForwardProp(X, parameters):\n",
    "    '''\n",
    "    Compute the prediction matrix A3.\n",
    "    --------\n",
    "    Input\n",
    "            - X : Matrix of input (n_x, m)\n",
    "            - parameters : dictionnary of parameters W and b, for each layers\n",
    "    Output\n",
    "            - A3 : The prediction matrix (n_y, m)\n",
    "            - cache : Dictionnary of the A and Z, to use them during backprop\n",
    "    '''\n",
    "    # get the parameters from the parameters dict\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # compute forward propagation\n",
    "    # first layer\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    # second layer\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = np.tanh(Z2)\n",
    "    # last layer\n",
    "    Z3 = W3.dot(A2) + b3\n",
    "    A3 = Sigmoid(Z3)\n",
    "    \n",
    "    # get the Z and A in to the cache dictionnary\n",
    "    cache = {}\n",
    "    cache['Z1'] = Z1\n",
    "    cache['A1'] = A1\n",
    "    cache['Z2'] = Z2\n",
    "    cache['A2'] = A2\n",
    "    \n",
    "    return A3, cache\n",
    "\n",
    "def ComputeCost(Y, A3):\n",
    "    '''\n",
    "    Compute the cost function.\n",
    "    --------\n",
    "    Input\n",
    "            - Y : Target matrix (n_y, m)\n",
    "            - A3 : Prediction matrix (n_y, m)\n",
    "    Output\n",
    "            - cost : the cost function computed for Y and A3\n",
    "    '''\n",
    "    # compute the loss matrix \n",
    "    loss = - Y * np.log(A3) - (1-Y) * np.log(1 - A3)\n",
    "    # sum the loss through the m examples\n",
    "    cost = np.average(loss)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def BackProp(X, Y, A3, parameters, cache):\n",
    "    '''\n",
    "    Compute the gradients of the cost for the parameters W, b of each layers\n",
    "    --------\n",
    "    Input\n",
    "            - X :\n",
    "            - Y :\n",
    "            - A3 :\n",
    "            - parameters : \n",
    "            - cache :\n",
    "    Output\n",
    "            - grads : dictionnary of the derivatives of the cost function\n",
    "                      for each parameters\n",
    "    '''\n",
    "    # m = number of training examples\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # get the A and Z from cache\n",
    "    Z1 = cache['Z1']\n",
    "    A1 = cache['A1']\n",
    "    Z2 = cache['Z2']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # get the W parameters\n",
    "    W3 = parameters['W3']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # last layer\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    # second layer\n",
    "    dA2 = W3.T.dot(dZ3)\n",
    "    dZ2 = (1 - np.tanh(Z2)**2) * dA2\n",
    "    dW2 = (1/m) * dZ2.dot(A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    # first layer\n",
    "    dA1 = W2.T.dot(dZ2)\n",
    "    dZ1 = (1 - np.tanh(Z1)**2) * dA1\n",
    "    dW1 = (1/m) * dZ1.dot(X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {}\n",
    "    grads['dW1'] = dW1\n",
    "    grads['db1'] = db1\n",
    "    grads['dW2'] = dW2\n",
    "    grads['db2'] = db2\n",
    "    grads['dW3'] = dW3\n",
    "    grads['db3'] = db3\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def UpdateParameters(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    Update the parameters by gradient descent\n",
    "    ---------\n",
    "    Input\n",
    "            - parameters : dictionnary of parameters W, b of each layer\n",
    "            - grads : dictionnary of gradient of the cost function\n",
    "                      for each parameters W, b of each leayer\n",
    "            - learning_rate : learning rate to use for updating the parameters\n",
    "    Output\n",
    "            - parameters : parameters updated after gradient descent\n",
    "    '''\n",
    "    # get parameters\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # get gradients\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    dW3 = grads['dW3']\n",
    "    db3 = grads['db3']\n",
    "    \n",
    "    # update the parameters    \n",
    "    parameters['W1'] = W1 - learning_rate * dW1\n",
    "    parameters['b1'] = b1 - learning_rate * db1\n",
    "    parameters['W2'] = W2 - learning_rate * dW2\n",
    "    parameters['b2'] = b2 - learning_rate * db2\n",
    "    parameters['W3'] = W3 - learning_rate * dW3\n",
    "    parameters['b3'] = b3 - learning_rate * db3\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def NN_model(X, Y, n_1, n_2, epoch=10000, learning_rate=0.01, verbose=True):\n",
    "    '''\n",
    "    Train a Neural Network of 3 layers (2 hidden layers Tanh and sigmoid for the output).\n",
    "    ----------\n",
    "    Input\n",
    "            - X : input training dataset (m, n_x)\n",
    "            - Y : target of the training dataset (m, 1)\n",
    "            - layer_units : tuple of number of units for the 2 ReLU layers\n",
    "            - epoch : number of iteration\n",
    "            - learning_rate : learning rate for the gradient descent\n",
    "            - verbose : if True, print cost function value every 100 epoch\n",
    "    Output\n",
    "            - parameters : dictionnary of the trained parameters W, b for each layers\n",
    "    '''\n",
    "    t0 = time.time()\n",
    "    # transpose X and Y\n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "    \n",
    "    # get the number of features n_x and number of examples m of X\n",
    "    n_x, m = X.shape\n",
    "    \n",
    "    # initialize the parameters\n",
    "    parameters = InitializeParameters(n_x, n_1, n_2, 1)\n",
    "    \n",
    "    # initialize a list to plot the evolution of the cost function\n",
    "    cost_list = []\n",
    "    for i in range(epoch):\n",
    "        # compute the forward propagation\n",
    "        A3, cache = ForwardProp(X, parameters)\n",
    "        \n",
    "        # compute the back propagation\n",
    "        grads = BackProp(X, Y, A3, parameters, cache)\n",
    "        \n",
    "        # update the parameters\n",
    "        parameters = UpdateParameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if  i%10 == 0:\n",
    "            # compute the cost function\n",
    "            cost = ComputeCost(Y, A3)\n",
    "            cost_list.append(cost)\n",
    "            \n",
    "            if verbose and (i%1000 == 0):\n",
    "                print('Cost function after epoch {} : {}'.format(i, cost))\n",
    "    \n",
    "    print('Cost function after epoch {} : {}'.format(epoch, cost))\n",
    "    print('Time : %.3f s' % (time.time()-t0))\n",
    "    \n",
    "    # print the cost function for each iterations\n",
    "    plt.plot(cost_list)\n",
    "    plt.title('Cost function')\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Cost Function')\n",
    "    \n",
    "    return parameters, cost_list\n",
    "\n",
    "def MakePrediction(X, parameters):\n",
    "    '''\n",
    "    Make prediction of the data X\n",
    "    ---------\n",
    "    Input\n",
    "            - X : Input data (m, n_x)\n",
    "            - parameters : parameters W, b of each layers of the NN model\n",
    "    Output\n",
    "            - Y_pred : Predicted labels for X (m, n_y)\n",
    "    '''\n",
    "    X = X.T\n",
    "    A3, _ = ForwardProp(X, parameters)\n",
    "    Y_pred = (A3 > 0.5)*1\n",
    "    Y_pred = Y_pred.T # transpose the prediction to get the usual form (m, n_y)\n",
    "    \n",
    "    return Y_pred \n",
    "\n",
    "def GetAccuracy(Y, Y_pred):\n",
    "    '''\n",
    "    Compute the accuracy\n",
    "    --------\n",
    "    Input\n",
    "            - Y : Target labels (m, n_y)\n",
    "            - Y_pred : Predicted labels (m, n_y)\n",
    "    Output\n",
    "            - accuracy : accuracy value\n",
    "    '''\n",
    "    accuracy = np.average(Y == Y_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on digits\n",
    "\n",
    "Test of the NN on the digits dataset from scikit-learn. We'll try to predict if the picture is **1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (1797, 64)\n",
      "Y shape : (1797, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits['data']/16 # normalize the values between 0 and 1\n",
    "Y = digits['target']\n",
    "Y = (Y == 1)*1  # target value =1 for digit 1, and 0 for all other digits\n",
    "\n",
    "Y = Y.reshape(-1, 1)\n",
    "print('X shape :', X.shape)\n",
    "print('Y shape :', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS8AAAE/CAYAAAAXCEVDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1377fd0a240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "index = 12\n",
    "digit = X[index].reshape(8,8)\n",
    "target = Y[index].squeeze()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(5,5)\n",
    "ax.imshow(digit, cmap='gray_r')\n",
    "ax.set_title('y = {}'.format(target))\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model to predict images of **1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function after epoch 0 : 0.6931467373746464\n",
      "Cost function after epoch 1000 : 0.3403216870700217\n",
      "Cost function after epoch 2000 : 0.060395094758117204\n",
      "Cost function after epoch 3000 : 0.031245351440609\n",
      "Cost function after epoch 4000 : 0.02146101578023537\n",
      "Cost function after epoch 5000 : 0.016894356153856288\n",
      "Time : 7.218 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nscI3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1377dcb8160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_1, n_2 = 20, 10\n",
    "\n",
    "parameters, cost_list = NN_model(X_train, Y_train, n_1, n_2, epoch=5000, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set accuracy : 0.99652052888\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      1283\n",
      "          1       0.99      0.98      0.98       154\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_train_pred = MakePrediction(X_train, parameters)\n",
    "accuracy = GetAccuracy(Y_train, Y_train_pred)\n",
    "\n",
    "print('train set accuracy :', accuracy)\n",
    "print(classification_report(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy : 0.986111111111\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99       332\n",
      "          1       0.85      1.00      0.92        28\n",
      "\n",
      "avg / total       0.99      0.99      0.99       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = MakePrediction(X_test, parameters)\n",
    "accuracy = GetAccuracy(Y_test, Y_test_pred)\n",
    "\n",
    "print('test set accuracy :', accuracy)\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Very good performance on this small dataset !\n",
    "Next : implement a softmax classifier for the output, to predict every digits possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
